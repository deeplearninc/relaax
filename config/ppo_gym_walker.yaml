---

# This is an example of configuration to train PPO L-BFGS with gym's BipedalWalker-v2.
#
# To run this training navigate to empty directory next to relaax repo,
# open three terminals there and run:
# relaax-parameter-server --config ../relaax/config/ppo_gym_walker.yaml
# relaax-rlx-server --config ../relaax/config/ppo_gym_walker.yaml
# ../relaax/environments/OpenAI_Gym/main --rlx-server localhost:7001 --env BipedalWalker-v2

relaax-parameter-server:
  --bind: localhost:7000
  --checkpoint-dir: checkpoints/ppo_gym_walker
  --log-level: WARNING
  --metrics-dir: metrics_ppo_gym_walker

relaax-rlx-server:
  --bind: 0.0.0.0:7001
  --parameter-server: localhost:7000
  --log-level: WARNING

algorithm:
  path: ../relaax/algorithms/ppo_lbfgs

  action_size: 4                  # action size for the given environment
  state_size: [24]                # array of dimensions for the input observation
  hidden_layers_sizes: [64, 64]   # number of hidden units for MLP
  activation: tanh                # activation function for MLP
  use_filter: false               # use average filter of the incoming observations and rewards
  discrete: false                 # switch between continuous and discrete action spaces
  async: false                    # set to true to collect experience without blocking the updater

  PG_OPTIONS:
    timestep_limit: 1600          # length in steps for one round in environment
    n_iter: 10000                 # number of updates to pass through the training (training length)
    timesteps_per_batch: 10000    # number of experience to collect before update
    rewards_gamma: 0.995          # rewards discount factor
    gae_lambda: 0.97              # lambda from generalized advantage estimation

  PPO:
    kl_target: 0.01               # desired kl divergence between old and new policy
    reverse_kl: 0                 # kl[new, old] instead of kl[old, new]
    maxiter: 25                   # maximum number of iterations
