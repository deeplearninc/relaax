---

# This is an example of configuration to train DA3C Continuous with gym's BipedalWalker-v2.
#
# To run this training navigate to empty directory next to relaax repo,
# open three terminals there and run:
# relaax-parameter-server --config ../relaax/config/da3cc_gym_walker.yaml
# relaax-rlx-server --config ../relaax/config/da3cc_gym_walker.yaml
# ../relaax/environments/OpenAI_Gym/main --rlx-server localhost:7001 --env BipedalWalker-v2

relaax-parameter-server:
  --bind: localhost:7000
  --checkpoint-dir: checkpoints/a3cc_gym_walker
  --log-level: WARNING
  --metrics-dir: metrics_a3cc_gym_walker

relaax-rlx-server:
  --bind: 0.0.0.0:7001
  --parameter-server: localhost:7000
  --log-level: WARNING

algorithm:
  path: ../relaax/algorithms/da3c_cont

  action_size: 4                  # action size for the given environment
#  action_low: [-1, -1, -1, -1]    # minimum value for each action (length should be equal to action_size)
#  action_high: [1, 1, 1, 1]       # maximum value for each action (length should be equal to action_size)
  state_size: [24]                # array of dimensions for the input observation
  history_len: 1                  # number of observations to stack in state
  episode_len: 5                  # local loop size for one episode
  loss_type: 1                    # it has 2 options for type of loss function: A or B (B is set by default)
  gpu: false                      # to use GPU, set to the True
  lstm: false                     # to use LSTM instead of FF, set to the True
  max_global_step: 1e8            # amount of maximum global steps to pass through the training
  reward_count_interval: 0        # useful for non-episodic (non-terminal) environments, it prints out the accumalated
                                  # reward each defined interval, for episodic environments it is recommended to set
                                  # this to 0 to print out accumulated reward at each episode end
  initial_learning_rate: 1e-4     # 5e-5
  entropy_beta: 1e-3              # entropy regularization constant
  rewards_gamma: 0.99             # rewards discount factor
  RMSProp:
    decay: 0.99
    epsilon: 0.1
    gradient_norm_clipping: 40
